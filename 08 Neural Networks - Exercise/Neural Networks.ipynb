{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Neural Networks\n",
    "In this exercise, several parts of the code are missing, which should be completed by you. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import sklearn\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import tqdm\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score\n",
    "sns.set()\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP for Skin disease dataset using `scikit-learn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us apply a neural network on the skin disesase data. To reduce the training time we reduce the amount of data in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>t0</th>\n",
       "      <th>t1</th>\n",
       "      <th>t2</th>\n",
       "      <th>t3</th>\n",
       "      <th>t4</th>\n",
       "      <th>t5</th>\n",
       "      <th>t6</th>\n",
       "      <th>t7</th>\n",
       "      <th>t8</th>\n",
       "      <th>t9</th>\n",
       "      <th>t10</th>\n",
       "      <th>t11</th>\n",
       "      <th>t12</th>\n",
       "      <th>t13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>399763</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>147.006546</td>\n",
       "      <td>143.778107</td>\n",
       "      <td>154.446762</td>\n",
       "      <td>0.683331</td>\n",
       "      <td>1.200395</td>\n",
       "      <td>4.162486</td>\n",
       "      <td>0.342579</td>\n",
       "      <td>0.438565</td>\n",
       "      <td>-1.637385</td>\n",
       "      <td>0.931025</td>\n",
       "      <td>1.535766</td>\n",
       "      <td>5.316552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179351</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>148.038666</td>\n",
       "      <td>141.652588</td>\n",
       "      <td>151.419388</td>\n",
       "      <td>1.005779</td>\n",
       "      <td>0.772486</td>\n",
       "      <td>3.608217</td>\n",
       "      <td>-0.598483</td>\n",
       "      <td>0.419350</td>\n",
       "      <td>1.930153</td>\n",
       "      <td>1.399444</td>\n",
       "      <td>1.031002</td>\n",
       "      <td>4.742771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489030</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>146.840973</td>\n",
       "      <td>140.802795</td>\n",
       "      <td>138.360687</td>\n",
       "      <td>1.595206</td>\n",
       "      <td>2.676929</td>\n",
       "      <td>5.883652</td>\n",
       "      <td>-1.607611</td>\n",
       "      <td>-2.674804</td>\n",
       "      <td>5.202581</td>\n",
       "      <td>2.284131</td>\n",
       "      <td>3.653879</td>\n",
       "      <td>8.494467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225829</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>146.447952</td>\n",
       "      <td>145.673996</td>\n",
       "      <td>125.780487</td>\n",
       "      <td>1.557866</td>\n",
       "      <td>2.570986</td>\n",
       "      <td>6.929062</td>\n",
       "      <td>-1.375071</td>\n",
       "      <td>-2.344707</td>\n",
       "      <td>6.854708</td>\n",
       "      <td>2.102766</td>\n",
       "      <td>3.587329</td>\n",
       "      <td>10.010593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243109</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>145.647827</td>\n",
       "      <td>146.265915</td>\n",
       "      <td>159.128494</td>\n",
       "      <td>0.985775</td>\n",
       "      <td>1.698258</td>\n",
       "      <td>5.269435</td>\n",
       "      <td>-0.727311</td>\n",
       "      <td>-1.214882</td>\n",
       "      <td>-4.540771</td>\n",
       "      <td>1.268871</td>\n",
       "      <td>2.254031</td>\n",
       "      <td>7.528327</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        class   t0   t1          t2          t3          t4        t5  \\\n",
       "399763      0  0.0  1.0  147.006546  143.778107  154.446762  0.683331   \n",
       "179351      0  0.0  1.0  148.038666  141.652588  151.419388  1.005779   \n",
       "489030      1  0.0  1.0  146.840973  140.802795  138.360687  1.595206   \n",
       "225829      0  0.0  1.0  146.447952  145.673996  125.780487  1.557866   \n",
       "243109      1  0.0  1.0  145.647827  146.265915  159.128494  0.985775   \n",
       "\n",
       "              t6        t7        t8        t9       t10       t11       t12  \\\n",
       "399763  1.200395  4.162486  0.342579  0.438565 -1.637385  0.931025  1.535766   \n",
       "179351  0.772486  3.608217 -0.598483  0.419350  1.930153  1.399444  1.031002   \n",
       "489030  2.676929  5.883652 -1.607611 -2.674804  5.202581  2.284131  3.653879   \n",
       "225829  2.570986  6.929062 -1.375071 -2.344707  6.854708  2.102766  3.587329   \n",
       "243109  1.698258  5.269435 -0.727311 -1.214882 -4.540771  1.268871  2.254031   \n",
       "\n",
       "              t13  \n",
       "399763   5.316552  \n",
       "179351   4.742771  \n",
       "489030   8.494467  \n",
       "225829  10.010593  \n",
       "243109   7.528327  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"skin_disease.csv\")\n",
    "df = df.sample(frac=1)\n",
    "df = df.iloc[0:100000]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=[\"class\"])\n",
    "y = df[\"class\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Split the data into a train and test set. Use 40% of the data for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define our Multi Layer Perceptron with 2 hidden layers. This time we use the [MLPClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html) implementation from Scikit-Learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(30,15),\n",
    "                    activation='relu',  # activation function\n",
    "                    solver='adam',  # optimizer\n",
    "                    batch_size=1024)  # size of minibatches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Train the neural network on `X_train`, `y_train` and plot the loss by accessing the attribute `loss_curve_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_costs(costs):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(costs)\n",
    "    ax.set_title(\"Loss curve\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "mlp.fit(X_train, y_train)\n",
    "plot_costs(mlp.loss_curve_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "mlp.fit(X_train, y_train)\n",
    "plot_costs(mlp.loss_curve_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Implement your own predict function. For that we'll need an activation function for the hidden layers, in our case `relu` and for the output layer `sigmoid`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    ### START YOUR CODE ###\n",
    "    return np.maximum(0, x)\n",
    "    #if x > 0:\n",
    "    #    return x\n",
    "    #else:\n",
    "    #    return 0\n",
    "    ### END YOUR CODE ###\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    ### START YOUR CODE ###\n",
    "    return (1/(1+np.exp(-x)))\n",
    "    ### END YOUR CODE ###\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "def predict(mlp, X):\n",
    "    ### START YOUR CODE ###\n",
    "    A = X\n",
    "    \n",
    "    # forward propagate through layers\n",
    "    for i, (W, B) in enumerate(zip(mlp.coefs_, mlp.intercepts_)):\n",
    "        z = A.dot(W) + B\n",
    "        # if hidden layer, apply `relu`\n",
    "        if i != mlp.n_layers_ - 2:\n",
    "            A = relu(z)\n",
    "            \n",
    "    # last layer output activation `sigmoid`\n",
    "    out = sigmoid(z)  \n",
    "    # transform to 1-D and threshold\n",
    "    out = np.squeeze(out)\n",
    "    out = np.array(out > 0.5, dtype=int)\n",
    "    \n",
    "    return out\n",
    "    ### END YOUR CODE ###\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "def predict(mlp, X):\n",
    "    # define the first activations, e.g. inputs\n",
    "    A = X\n",
    "    \n",
    "    # forward propagate through layers\n",
    "    for i, (W, B) in enumerate(zip(mlp.coefs_, mlp.intercepts_)):\n",
    "        z = A.dot(W) + B\n",
    "        # if hidden layer, apply `relu`\n",
    "        if i != mlp.n_layers_ - 2:\n",
    "            A = relu(z)\n",
    "            \n",
    "    # last layer output activation `sigmoid`\n",
    "    out = sigmoid(z)  \n",
    "    # transform to 1-D and threshold\n",
    "    out = np.squeeze(out)\n",
    "    out = np.array(out > 0.5, dtype=int)\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Test your implementation with the scikit-learn predict function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are the outputs the same: True\n"
     ]
    }
   ],
   "source": [
    "y_pred_scikit = mlp.predict(X_test)\n",
    "y_pred_own = predict(mlp, X_test.values)\n",
    "\n",
    "print('Are the outputs the same: %s' % (y_pred_scikit == y_pred_own).all() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "y_pred_scikit = mlp.predict(X_test)\n",
    "y_pred_own = predict(mlp, X_test.values)\n",
    "\n",
    "print('Are the outputs the same: %s' % (y_pred_scikit == y_pred_own).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Predict the values on the test set and calculate the accuracy and the f1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9568\n",
      "F1: 0.7548\n"
     ]
    }
   ],
   "source": [
    "y_pred = predict(mlp, X_test.values)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy: %.4f\" % accuracy)\n",
    "print(\"F1: %.4f\" % f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "y_pred = predict(mlp, X_test.values)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy: %.4f\" % accuracy)\n",
    "print(\"F1: %.4f\" % f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP for Skin disease dataset using `TensorFlow`\n",
    "In practice, the MLP from `scikit-learn` is never used because of the lack of customisation and the absence of GPU training. `TensorFlow` is a library specialised in deep learning and therefore also has implementations for advanced techniques. Thus the section below is a quick introduction to how the same network can be implemented using `TensorFlow`. The networks' results do not need to be the same, since as mentioned above, the `scikit-learn` implementation can not be as customised as the `TensorFlow` one. \n",
    "\n",
    "If `TensorFlow` is not already installed on the environment, it can be done using the \"magic\" cell from below. If it is already installed, make sure to use version `2.3.1`. Higher versions should also work but weren't tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow==2.3.1\u001b[0m\r\n",
      "\u001b[31mERROR: No matching distribution found for tensorflow==2.3.1\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow==2.3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "#assert tf.__version__ == '2.3.1'\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A model in `TensorFlow` can be implemented using the `Sequential API`, which enables for easy extensibility by calling `.add()`. To implement the same MLP as above, we can sequentially add `Dense` layers to the model. Here the customization possibilities compared to `scikit-learn` is evident. For example, the activation function can be set for each layer separately, which was impossible before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 30)                450       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 15)                465       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 16        \n",
      "=================================================================\n",
      "Total params: 931\n",
      "Trainable params: 931\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "dataset_dim = X_train.shape[1]\n",
    "\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Dense(30, input_shape=(dataset_dim, ), activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(15, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After defining the model, it needs to be compiled using an optimizer and loss function. In our case, we'll use adam as optimizer and binary cross-entropy as loss. Now the model can be trained by specifying the number of epochs and the batch size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "Epoch 1/150\n",
      "60000/60000 [==============================] - 0s 5us/sample - loss: 1.3811 - accuracy: 0.8139\n",
      "Epoch 2/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.3723 - accuracy: 0.8989\n",
      "Epoch 3/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.3024 - accuracy: 0.9094\n",
      "Epoch 4/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.2633 - accuracy: 0.9122\n",
      "Epoch 5/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.2345 - accuracy: 0.9145\n",
      "Epoch 6/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.2150 - accuracy: 0.9197\n",
      "Epoch 7/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.2037 - accuracy: 0.9232\n",
      "Epoch 8/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1963 - accuracy: 0.9259\n",
      "Epoch 9/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1909 - accuracy: 0.9284\n",
      "Epoch 10/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1861 - accuracy: 0.9305\n",
      "Epoch 11/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1814 - accuracy: 0.9315\n",
      "Epoch 12/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1774 - accuracy: 0.9328\n",
      "Epoch 13/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1741 - accuracy: 0.9343\n",
      "Epoch 14/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1724 - accuracy: 0.9355\n",
      "Epoch 15/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1693 - accuracy: 0.9362\n",
      "Epoch 16/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1676 - accuracy: 0.9361\n",
      "Epoch 17/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1652 - accuracy: 0.9368\n",
      "Epoch 18/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1637 - accuracy: 0.9372\n",
      "Epoch 19/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1629 - accuracy: 0.9373\n",
      "Epoch 20/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1630 - accuracy: 0.9365\n",
      "Epoch 21/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1613 - accuracy: 0.9379\n",
      "Epoch 22/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1580 - accuracy: 0.9387\n",
      "Epoch 23/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1572 - accuracy: 0.9388\n",
      "Epoch 24/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1554 - accuracy: 0.9395\n",
      "Epoch 25/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1548 - accuracy: 0.9393\n",
      "Epoch 26/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1537 - accuracy: 0.9394\n",
      "Epoch 27/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1544 - accuracy: 0.9391\n",
      "Epoch 28/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1538 - accuracy: 0.9387\n",
      "Epoch 29/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1510 - accuracy: 0.9406\n",
      "Epoch 30/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1510 - accuracy: 0.9401\n",
      "Epoch 31/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1513 - accuracy: 0.9395\n",
      "Epoch 32/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1536 - accuracy: 0.9384\n",
      "Epoch 33/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1500 - accuracy: 0.9397\n",
      "Epoch 34/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1481 - accuracy: 0.9412\n",
      "Epoch 35/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1480 - accuracy: 0.9408\n",
      "Epoch 36/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1490 - accuracy: 0.9404\n",
      "Epoch 37/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1493 - accuracy: 0.9403\n",
      "Epoch 38/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1462 - accuracy: 0.9413\n",
      "Epoch 39/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1469 - accuracy: 0.9412\n",
      "Epoch 40/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1474 - accuracy: 0.9411\n",
      "Epoch 41/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1459 - accuracy: 0.9413\n",
      "Epoch 42/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1463 - accuracy: 0.9410\n",
      "Epoch 43/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1475 - accuracy: 0.9409\n",
      "Epoch 44/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1493 - accuracy: 0.9403\n",
      "Epoch 45/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1473 - accuracy: 0.9407\n",
      "Epoch 46/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1439 - accuracy: 0.9420\n",
      "Epoch 47/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1438 - accuracy: 0.9424\n",
      "Epoch 48/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1447 - accuracy: 0.9417\n",
      "Epoch 49/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1456 - accuracy: 0.9420\n",
      "Epoch 50/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1435 - accuracy: 0.9423\n",
      "Epoch 51/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1420 - accuracy: 0.9424\n",
      "Epoch 52/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1421 - accuracy: 0.9433\n",
      "Epoch 53/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1423 - accuracy: 0.9430\n",
      "Epoch 54/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1418 - accuracy: 0.9423\n",
      "Epoch 55/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1413 - accuracy: 0.9431\n",
      "Epoch 56/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1416 - accuracy: 0.9424\n",
      "Epoch 57/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1428 - accuracy: 0.9427\n",
      "Epoch 58/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1421 - accuracy: 0.9426\n",
      "Epoch 59/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1417 - accuracy: 0.9426\n",
      "Epoch 60/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1407 - accuracy: 0.9434\n",
      "Epoch 61/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1418 - accuracy: 0.9425\n",
      "Epoch 62/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1387 - accuracy: 0.9437\n",
      "Epoch 63/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1423 - accuracy: 0.9424\n",
      "Epoch 64/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1409 - accuracy: 0.9435\n",
      "Epoch 65/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1378 - accuracy: 0.9449\n",
      "Epoch 66/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1402 - accuracy: 0.9444\n",
      "Epoch 67/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1393 - accuracy: 0.9437\n",
      "Epoch 68/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1383 - accuracy: 0.9446\n",
      "Epoch 69/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1376 - accuracy: 0.9443\n",
      "Epoch 70/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1369 - accuracy: 0.9454\n",
      "Epoch 71/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1386 - accuracy: 0.9439\n",
      "Epoch 72/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1405 - accuracy: 0.9442\n",
      "Epoch 73/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1391 - accuracy: 0.9441\n",
      "Epoch 74/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1402 - accuracy: 0.9433\n",
      "Epoch 75/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1366 - accuracy: 0.9448\n",
      "Epoch 76/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1369 - accuracy: 0.9446\n",
      "Epoch 77/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1359 - accuracy: 0.9451\n",
      "Epoch 78/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1359 - accuracy: 0.9454\n",
      "Epoch 79/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1361 - accuracy: 0.9450\n",
      "Epoch 80/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1360 - accuracy: 0.9451\n",
      "Epoch 81/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1363 - accuracy: 0.9455\n",
      "Epoch 82/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1361 - accuracy: 0.9443\n",
      "Epoch 83/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1359 - accuracy: 0.9446\n",
      "Epoch 84/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1349 - accuracy: 0.9457\n",
      "Epoch 85/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1353 - accuracy: 0.9459\n",
      "Epoch 86/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1349 - accuracy: 0.9453\n",
      "Epoch 87/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1349 - accuracy: 0.9459\n",
      "Epoch 88/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1353 - accuracy: 0.9456\n",
      "Epoch 89/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1339 - accuracy: 0.9459\n",
      "Epoch 90/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1363 - accuracy: 0.9444\n",
      "Epoch 91/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1336 - accuracy: 0.9459\n",
      "Epoch 92/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1362 - accuracy: 0.9451\n",
      "Epoch 93/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1367 - accuracy: 0.9452\n",
      "Epoch 94/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1362 - accuracy: 0.9455\n",
      "Epoch 95/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1336 - accuracy: 0.9458\n",
      "Epoch 96/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1339 - accuracy: 0.9455\n",
      "Epoch 97/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1334 - accuracy: 0.9462\n",
      "Epoch 98/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1334 - accuracy: 0.9461\n",
      "Epoch 99/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1333 - accuracy: 0.9460\n",
      "Epoch 100/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1330 - accuracy: 0.9463\n",
      "Epoch 101/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1332 - accuracy: 0.9465\n",
      "Epoch 102/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1340 - accuracy: 0.9457\n",
      "Epoch 103/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1312 - accuracy: 0.9472\n",
      "Epoch 104/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1329 - accuracy: 0.9463\n",
      "Epoch 105/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1329 - accuracy: 0.9462\n",
      "Epoch 106/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1365 - accuracy: 0.9455\n",
      "Epoch 107/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1331 - accuracy: 0.9456\n",
      "Epoch 108/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1326 - accuracy: 0.9464\n",
      "Epoch 109/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1325 - accuracy: 0.9464\n",
      "Epoch 110/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1311 - accuracy: 0.9470\n",
      "Epoch 111/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1312 - accuracy: 0.9469\n",
      "Epoch 112/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1313 - accuracy: 0.9468\n",
      "Epoch 113/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1316 - accuracy: 0.9468\n",
      "Epoch 114/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1305 - accuracy: 0.9479\n",
      "Epoch 115/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1310 - accuracy: 0.9465\n",
      "Epoch 116/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1295 - accuracy: 0.9479\n",
      "Epoch 117/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1301 - accuracy: 0.9475\n",
      "Epoch 118/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1304 - accuracy: 0.9470\n",
      "Epoch 119/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1303 - accuracy: 0.9471\n",
      "Epoch 120/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1307 - accuracy: 0.9470\n",
      "Epoch 121/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1317 - accuracy: 0.9468\n",
      "Epoch 122/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1330 - accuracy: 0.9456\n",
      "Epoch 123/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1300 - accuracy: 0.9470\n",
      "Epoch 124/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1312 - accuracy: 0.9466\n",
      "Epoch 125/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1299 - accuracy: 0.9479\n",
      "Epoch 126/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1281 - accuracy: 0.9483\n",
      "Epoch 127/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1305 - accuracy: 0.9477\n",
      "Epoch 128/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1293 - accuracy: 0.9477\n",
      "Epoch 129/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1292 - accuracy: 0.9484\n",
      "Epoch 130/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1285 - accuracy: 0.9477\n",
      "Epoch 131/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1312 - accuracy: 0.9468\n",
      "Epoch 132/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1287 - accuracy: 0.9480\n",
      "Epoch 133/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1289 - accuracy: 0.9481\n",
      "Epoch 134/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1288 - accuracy: 0.9478\n",
      "Epoch 135/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1295 - accuracy: 0.9480\n",
      "Epoch 136/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1289 - accuracy: 0.9485\n",
      "Epoch 137/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1295 - accuracy: 0.9482\n",
      "Epoch 138/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1287 - accuracy: 0.9484\n",
      "Epoch 139/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1283 - accuracy: 0.9491\n",
      "Epoch 140/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1291 - accuracy: 0.9482\n",
      "Epoch 141/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1273 - accuracy: 0.9489\n",
      "Epoch 142/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1287 - accuracy: 0.9477\n",
      "Epoch 143/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1296 - accuracy: 0.9476\n",
      "Epoch 144/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1260 - accuracy: 0.9501\n",
      "Epoch 145/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1266 - accuracy: 0.9497\n",
      "Epoch 146/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1263 - accuracy: 0.9494\n",
      "Epoch 147/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1281 - accuracy: 0.9489\n",
      "Epoch 148/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1259 - accuracy: 0.9500\n",
      "Epoch 149/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1274 - accuracy: 0.9490\n",
      "Epoch 150/150\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 0.1284 - accuracy: 0.9489\n"
     ]
    }
   ],
   "source": [
    "adam = tf.keras.optimizers.Adam()\n",
    "model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, y_train, batch_size=1024, epochs=150) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot the loss curve using the same function as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_costs(history.history[\"loss\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model can also be evaluated on the test set with familiar code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9488\n",
      "F1: 0.6817\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_pred = np.array(y_pred > 0.5, dtype=int).squeeze()\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy: %.4f\" % accuracy)\n",
    "print(\"F1: %.4f\" % f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
